{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd82417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2131061it [00:35, 60860.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0704.1267\n",
      "1         0704.2344\n",
      "2         0704.3500\n",
      "3         0704.3501\n",
      "4         0704.3520\n",
      "            ...    \n",
      "14515    cs/9912005\n",
      "14516    cs/9912007\n",
      "14517    cs/9912009\n",
      "14518    cs/9912014\n",
      "14519    cs/9912017\n",
      "Name: id, Length: 14520, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import arxiv\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "data_file = 'D:/Abhilasha/PennStateWORKSHEET/FALL2022/Capstone Project/DATA/arxiv-metadata-oai-snapshot.json/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "\"\"\"  Using Yield to avod any memory issues \"\"\"\n",
    "\n",
    "def get_metadata():\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "            \n",
    "            \n",
    "## only computer science category\n",
    "arxiv_categories = [\"cs.AI\", \n",
    "                    \"cs.CV\", \n",
    "                    \"cs.LG\",\n",
    "                     \"cs.CE\",\"cs.AR\",\"cs.CC\",\"cs.CG\",\n",
    "                    \"cs.CL\",\"cs.CR\",\"cs.CY\",\"cs.DB\",\n",
    "                    \"cs.DC\",\"cs.DL\",\"cs.DM\",\"cs.DS\",\"cs.ET\",\n",
    "                    \"cs.FL\",\"cs.GL\",\"cs.GR\",\"cs.GT\",\"cs.HC\",\n",
    "                    \"cs.IR\",\"cs.IT\",\"cs.LG\",\"cs.LO\",\"cs.MA\",\"cs.MM\",\"cs.MS\",\"cs.NA\",\"cs.NE\",\"cs.NI\",\n",
    "                    \"cs.OH\",\"cs.OS\",\"cs.PF\",\"cs.PL\",\"cs.RO\",\"cs.SC\",\"cs.SD\",\"cs.SE\",\"cs.SI\",\"cs.SY\"\n",
    "                    ] \n",
    "def downloadArxivPDF(idlist):\n",
    "    \n",
    "  \n",
    "    for idl in idlist:\n",
    "        ##item = idl+\"v1\"\n",
    "        retr_paper= arxiv.Search(id_list=[idl]).results()\n",
    "        retr_paper.download_pdf(dirpath=\"D:\\Abhilasha\\PennStateWORKSHEET\\FALL2022\\Capstone Project\\Output Files\", filename=idl+\"downloaded-paper.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(categories=arxiv_categories):\n",
    "    paperids = []\n",
    "    authors = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    metadata = get_metadata()\n",
    "    for paper in tqdm(metadata):\n",
    "        paper_dict = json.loads(paper)\n",
    "        category = paper_dict.get('categories')\n",
    "        if category in categories:\n",
    "            try:\n",
    "                year = int(paper_dict.get('journal-ref')[-4:])\n",
    "                titles.append(paper_dict.get('title'))\n",
    "                paperids.append(paper_dict.get('id'))\n",
    "                authors.append(paper_dict.get('authors'))\n",
    "                abstracts.append(paper_dict.get('abstract').replace(\"\\n\",\"\"))\n",
    "            except:\n",
    "                pass \n",
    "\n",
    "    papers = pd.DataFrame({'id':paperids,'title': titles,'authors':authors,'abstract': abstracts})\n",
    "    papers = papers.dropna()\n",
    "    \n",
    "    papers[\"title\"] = papers[\"title\"].apply(lambda x: re.sub('\\s+',' ', x))\n",
    "    papers[\"abstract\"] = papers[\"abstract\"].apply(lambda x: re.sub('\\s+',' ', x))\n",
    "   \n",
    "\n",
    "    \n",
    "    return papers\n",
    "\n",
    "papers = build_dataset(arxiv_categories)\n",
    "papers.to_csv('Arxiv-metadata.csv',encoding='utf-8')\n",
    "print(papers['id'])\n",
    "\n",
    "\n",
    "  \n",
    "# establish connections\n",
    "conn_string = 'postgres://postgres:pass@127.0.0.1/arxiv_database'\n",
    "  \n",
    "db = create_engine(conn_string)\n",
    "conn = db.connect()\n",
    "conn1 = psycopg2.connect(\n",
    "    database=\"arxiv_database\",\n",
    "  user='postgres', \n",
    "  password='pass', \n",
    "  host='127.0.0.1', \n",
    "  port= '5432'\n",
    ")\n",
    "  \n",
    "conn1.autocommit = True\n",
    "cursor = conn1.cursor()\n",
    "  \n",
    "# drop table if it already exists\n",
    "cursor.execute('drop table if exists arxiv_database')\n",
    "  \n",
    "sql = '''CREATE TABLE arxiv_database(id char(20) ,\n",
    "title char(20) ,authors char(20),abstract char(300));'''\n",
    "  \n",
    "cursor.execute(sql)\n",
    "\n",
    "  \n",
    "# converting papers dataframe to sql\n",
    "papers.to_sql('arxiv_database', conn, if_exists= 'replace')\n",
    "  \n",
    "# fetching all rows\n",
    "sql1='''select * from arxiv_database;'''\n",
    "cursor.execute(sql1)\n",
    "for i in cursor.fetchmany(10):\n",
    "    print(i)\n",
    "  \n",
    "conn1.commit()\n",
    "conn1.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f46044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb47340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689d532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
