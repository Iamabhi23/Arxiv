{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for training the models that support the semantic search capability in the streamlit apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NCR\\.pyenv\\pyenv-win\\versions\\3.10.8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the chosen dataset for the model you want to produce. For the abstract model it is the arxiv-metadata-oai-snapshot.json file in the data folder and for the model on the full papers it is the corpus_file in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_json('../data/arxiv-metadata-oai-snapshot/arxiv-metadata-oai-snapshot.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning all of the unwanted characters out of the strings. This will reduce the number of words that become one offs due to adding a dash or other special character and will allow the model to learn a better contextual representation of each word. This step is only for the abstracts model since this is already done in creating the corpus file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_clean(text):\n",
    "    text = \"\".join([x.lower() if x.isalnum() or x.isspace() else \" \" for x in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there are extra spaces left over or initially present between words so we need to strip those out as well since we will be splitting on spaces. This step is only for the abstracts model since this is already done in creating the corpus file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"clean\"] = meta.loc[:, \"abstract\"].apply(string_clean).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents need to be converted into a TaggedDocument object for Doc2Vec to train on them. In the Case of the Papers model this is already done when creating the corpus file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(meta[\"clean\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model. I have a machine with 32 cores so I set the workers to 28. Check your cores before setting the workers for your machine. If you max out your CPU you may notice that your computer will lock up until training is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, epochs = 200, vector_size=100, window=6, min_count=1, workers=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete we can save the model to be used in the streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/abstracts/archive_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Doc2Vec.load('../models/archive_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Training the Model we need to train the UMAP lower Dimensional representation of the vectors of the documents. To do that we use the command below. THis is the first step in doing the topic analysis. We chose 2 components so that the final reduction could be plotted in 2D for visualization in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_args = {'n_neighbors': 50,\n",
    "            'n_components': 2,\n",
    "            'metric': 'cosine'}\n",
    "umap_model = UMAP(**umap_args).fit(model.dv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After UMAP reduction the final vectors have to be clustered. To do this we use HDBSCAN. This is because density based methods have no relationship to the origin. they cluster based on what is the densest space. In embedding layers the initialization and the final orientation of the vectors can be somewhat randomly dispersed through out space. This method allows for that without failing to cluster based on the relationship of the cluster to the origin. You can read more about this in the documentation for HDBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_args = {'min_cluster_size': 50,\n",
    "                'metric': 'euclidean',\n",
    "                'cluster_selection_method': 'eom'}\n",
    "cluster = HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train both models we now hove our topic clusters and our 2d vectors for plotting so lets save the models and move over to the app for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/UMAP', 'wb') as f:\n",
    "    joblib.dump(umap_model, f)\n",
    "with open('../models/clusters', 'wb') as f:\n",
    "    joblib.dump(cluster, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that we are using joblib to save these models. THis is important because joblib uses a pickle function to save and that means in order to load the models you must be useing the same version of python tha tthey were trained in. __We are using python 3.10.8__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit ('3.10.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ab7af08f86e62764b52326a1101c48e81079ad8d19e871f453440249cb51a1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
